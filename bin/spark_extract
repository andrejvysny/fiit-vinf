#!/usr/bin/env bash
set -euo pipefail

# Wrapper script for Spark HTML extraction via Docker Compose
# Optimized for TB-scale processing with streaming writes

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.spark.yml"

# Check Docker availability
if ! command -v docker >/dev/null 2>&1; then
    echo "Error: Docker is required to run Spark jobs. Please install Docker." >&2
    exit 1
fi

# Check if --local flag is passed (fallback to Python extractor)
if [[ "${1:-}" == "--local" ]]; then
    shift
    echo "[spark_extract] Running local Python extractor (--local flag detected)"
    cd "${PROJECT_ROOT}"
    exec python -m extractor "$@"
fi

# Build command arguments for Spark job
SPARK_ARGS=""
SAMPLE=""
PARTITIONS="128"  # Increased default for better parallelism
FORCE=""
DRY_RUN=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --sample)
            SAMPLE="$2"
            SPARK_ARGS="$SPARK_ARGS --sample $2"
            shift 2
            ;;
        --partitions)
            PARTITIONS="$2"
            SPARK_ARGS="$SPARK_ARGS --partitions $2"
            shift 2
            ;;
        --force)
            FORCE="--force"
            SPARK_ARGS="$SPARK_ARGS --force"
            shift
            ;;
        --dry-run)
            DRY_RUN="--dry-run"
            SPARK_ARGS="$SPARK_ARGS --dry-run"
            shift
            ;;
        --config)
            # Config is already mounted at /opt/app/config.yml
            shift 2
            ;;
        *)
            SPARK_ARGS="$SPARK_ARGS $1"
            shift
            ;;
    esac
done

# Set memory based on data size - optimized for large-scale processing
if [[ -n "${SAMPLE}" ]] && [[ "${SAMPLE}" -le 500 ]]; then
    # Small sample: minimal resources
    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-2g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-1g}"
    export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-1g}"
elif [[ -n "${SAMPLE}" ]] && [[ "${SAMPLE}" -le 5000 ]]; then
    # Medium sample: moderate resources
    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-4g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-2g}"
    export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-2g}"
else
    # Large/full dataset: maximum resources
    # With distributed writes, we don't need huge maxResultSize
    # Only stats tuples are collected to driver
    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-6g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-4g}"
    export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-2g}"
fi

echo "========================================"
echo "Spark HTML Extractor (Streaming Mode)"
echo "========================================"
echo "Config: ${PROJECT_ROOT}/config.yml"
echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
echo "Max Result Size: ${SPARK_MAX_RESULT_SIZE}"
echo "Partitions: ${PARTITIONS}"
[[ -n "${SAMPLE}" ]] && echo "Sample Size: ${SAMPLE}"
[[ -n "${FORCE}" ]] && echo "Force: enabled"
[[ -n "${DRY_RUN}" ]] && echo "Dry Run: enabled"
echo ""
echo "Architecture:"
echo "  - Text files: written directly by workers"
echo "  - Entities: written via Spark DataFrame"
echo "  - Stats only: collected to driver"
echo "========================================"

# Run Spark job via Docker Compose with optimized configuration
cd "${PROJECT_ROOT}"
docker compose -f "${COMPOSE_FILE}" run --rm \
    -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
    -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
    -e SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE}" \
    spark bash -c "
        pip install --no-cache-dir --disable-pip-version-check -q -r /opt/app/requirements.txt &&
        /opt/spark/bin/spark-submit \
            --master local[*] \
            --driver-memory ${SPARK_DRIVER_MEMORY} \
            --conf spark.driver.maxResultSize=${SPARK_MAX_RESULT_SIZE} \
            --conf spark.sql.adaptive.enabled=true \
            --conf spark.sql.adaptive.coalescePartitions.enabled=true \
            --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
            --conf spark.kryoserializer.buffer.max=512m \
            --conf spark.memory.fraction=0.8 \
            --conf spark.memory.storageFraction=0.3 \
            --conf spark.local.dir=/tmp/spark \
            /opt/app/spark/main.py${SPARK_ARGS}
    "

echo "========================================"
echo "Spark job completed"
echo "========================================"