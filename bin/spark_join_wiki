#!/usr/bin/env bash
set -euo pipefail

# Wrapper script for Spark Wiki-HTML entity join

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.spark.yml"

# Check Docker availability
if ! command -v docker >/dev/null 2>&1; then
    echo "Error: Docker is required to run Spark jobs. Please install Docker." >&2
    exit 1
fi

# Parse arguments
ENTITIES="workspace/store/entities/entities.tsv"
WIKI_DIR="workspace/store/wiki"
JOIN_OUT="workspace/store/join"
ENTITIES_MAX_ROWS=""
PARTITIONS="64"
LOG_FILE="logs/wiki_join.jsonl"
SPARK_ARGS=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --entities)
            ENTITIES="$2"
            shift 2
            ;;
        --wiki)
            WIKI_DIR="$2"
            shift 2
            ;;
        --out)
            JOIN_OUT="$2"
            shift 2
            ;;
        --entities-max-rows)
            ENTITIES_MAX_ROWS="$2"
            SPARK_ARGS="$SPARK_ARGS --entities-max-rows $2"
            shift 2
            ;;
        --partitions)
            PARTITIONS="$2"
            SPARK_ARGS="$SPARK_ARGS --partitions $2"
            shift 2
            ;;
        --log)
            LOG_FILE="$2"
            shift 2
            ;;
        --dry-run)
            SPARK_ARGS="$SPARK_ARGS --dry-run"
            shift
            ;;
        *)
            echo "Unknown option: $1" >&2
            exit 1
            ;;
    esac
done

# Set memory for join operations
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-6g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-3g}"

echo "========================================"
echo "Spark Wiki-HTML Join"
echo "========================================"
echo "Entities: ${ENTITIES}"
echo "Wiki Dir: ${WIKI_DIR}"
echo "Output: ${JOIN_OUT}"
echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
echo "Partitions: ${PARTITIONS}"
[[ -n "${ENTITIES_MAX_ROWS}" ]] && echo "Max Entity Rows: ${ENTITIES_MAX_ROWS}"
echo "Log: ${LOG_FILE}"
echo "========================================"

# Run Spark job via Docker Compose
cd "${PROJECT_ROOT}"
docker compose -f "${COMPOSE_FILE}" run --rm \
    -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
    -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
    spark bash -c "
        pip install --no-cache-dir --disable-pip-version-check -q -r /opt/app/requirements.txt &&
        /opt/spark/bin/spark-submit \
            --master local[*] \
            --driver-memory ${SPARK_DRIVER_MEMORY} \
            --conf spark.sql.adaptive.enabled=true \
            --conf spark.sql.shuffle.partitions=${PARTITIONS} \
            /opt/app/spark/jobs/join_html_wiki.py \
            --entities /opt/app/${ENTITIES} \
            --wiki /opt/app/${WIKI_DIR} \
            --out /opt/app/${JOIN_OUT} \
            --log /opt/app/${LOG_FILE} \
            --partitions ${PARTITIONS} \
            ${SPARK_ARGS}
    "

echo "========================================"
echo "Wiki-HTML join completed"
echo "========================================"