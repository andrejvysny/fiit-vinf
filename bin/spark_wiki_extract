#!/usr/bin/env bash
set -euo pipefail

# Wrapper script for Spark Wikipedia dump extraction

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.spark.yml"

# Check Docker availability
if ! command -v docker >/dev/null 2>&1; then
    echo "Error: Docker is required to run Spark jobs. Please install Docker." >&2
    exit 1
fi

# Parse arguments
WIKI_IN="wiki_dump"
WIKI_OUT="workspace/store/wiki"
WIKI_MAX_PAGES=""
PARTITIONS="256"  # Increased default for large files
LOG_FILE="logs/wiki_extract.jsonl"
SPARK_ARGS=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --wiki-in)
            WIKI_IN="$2"
            shift 2
            ;;
        --out)
            WIKI_OUT="$2"
            shift 2
            ;;
        --wiki-max-pages)
            WIKI_MAX_PAGES="$2"
            SPARK_ARGS="$SPARK_ARGS --wiki-max-pages $2"
            shift 2
            ;;
        --partitions)
            PARTITIONS="$2"
            shift 2
            ;;
        --log)
            LOG_FILE="$2"
            shift 2
            ;;
        --dry-run)
            SPARK_ARGS="$SPARK_ARGS --dry-run"
            shift
            ;;
        *)
            echo "Unknown option: $1" >&2
            echo "Usage: $0 [--wiki-in DIR] [--out DIR] [--wiki-max-pages N] [--partitions N] [--log FILE] [--dry-run]"
            exit 1
            ;;
    esac
done

# Check if this is a test run or full run
if [[ -n "${WIKI_MAX_PAGES}" ]] && [[ "${WIKI_MAX_PAGES}" -le 1000 ]]; then
    # Test mode - lower memory
    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-4g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-2g}"
    echo "Test mode: Processing ${WIKI_MAX_PAGES} pages"
else
    # Full extraction - high memory for 100GB+ file
    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-12g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-6g}"
    echo "Full mode: Processing complete dump"
fi

echo "========================================"
echo "Spark Wikipedia Extractor"
echo "========================================"
echo "Wiki Input: ${WIKI_IN}"
echo "Output: ${WIKI_OUT}"
echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
echo "Partitions: ${PARTITIONS}"
[[ -n "${WIKI_MAX_PAGES}" ]] && echo "Max Pages: ${WIKI_MAX_PAGES}"
echo "Log: ${LOG_FILE}"
echo "========================================"

# Run Spark job via Docker Compose with increased memory limits
cd "${PROJECT_ROOT}"
docker compose -f "${COMPOSE_FILE}" run --rm \
    -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
    -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
    -v "${PROJECT_ROOT}/${WIKI_IN}:/opt/app/${WIKI_IN}:ro" \
    spark bash -c "
        pip install --no-cache-dir --disable-pip-version-check -q -r /opt/app/requirements.txt &&
        /opt/spark/bin/spark-submit \
            --master local[*] \
            --driver-memory ${SPARK_DRIVER_MEMORY} \
            --executor-memory ${SPARK_EXECUTOR_MEMORY} \
            --conf spark.driver.maxResultSize=4g \
            --conf spark.memory.offHeap.enabled=true \
            --conf spark.memory.offHeap.size=2g \
            --conf spark.sql.adaptive.enabled=true \
            --conf spark.sql.adaptive.coalescePartitions.enabled=true \
            --conf spark.sql.shuffle.partitions=${PARTITIONS} \
            /opt/app/spark/jobs/wiki_extractor.py \
            --wiki-in /opt/app/${WIKI_IN} \
            --out /opt/app/${WIKI_OUT} \
            --log /opt/app/${LOG_FILE} \
            ${SPARK_ARGS}
    "

echo "========================================"
echo "Wikipedia extraction completed"
echo "========================================"