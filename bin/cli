#!/usr/bin/env bash
set -euo pipefail

# =============================================================================
# VINF Pipeline CLI
# =============================================================================
# Unified command-line interface for the VINF data processing pipeline.
#
# Subcommands:
#   extract       - Extract text and entities from crawled GitHub HTML
#   wiki          - Extract structured data from Wikipedia dump
#   join          - Join extracted entities with Wikipedia data
#   lucene-build  - Build PyLucene index
#   lucene-search - Search the Lucene index
#   lucene-compare- Compare TF-IDF vs Lucene results
#   stats         - Show pipeline statistics
#
# Usage:
#   bin/cli <subcommand> [options]
#   bin/cli --help
#
# =============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.yml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# =============================================================================
# Helper Functions
# =============================================================================

print_header() {
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}"
}

print_success() {
    echo -e "${GREEN}✓ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}

print_error() {
    echo -e "${RED}✗ $1${NC}" >&2
}

check_docker() {
    if ! command -v docker >/dev/null 2>&1; then
        print_error "Docker is required. Please install Docker."
        exit 1
    fi
}

show_main_help() {
    cat <<EOF
VINF Pipeline CLI - Unified command-line interface

Usage: bin/cli <subcommand> [options]

Subcommands:
  extract        Extract text and entities from crawled GitHub HTML
  wiki           Extract structured data from Wikipedia dump
  join           Join extracted entities with Wikipedia data
  lucene-build   Build PyLucene index from extracted data
  lucene-search  Search the Lucene index
  lucene-compare Compare TF-IDF vs Lucene search results
  stats          Show pipeline statistics and generate reports

Global Options:
  -h, --help     Show help for main CLI or subcommand
  --version      Show version information

Examples:
  bin/cli extract --sample 100                    # Test extraction with 100 files
  bin/cli wiki --sample 10000                     # Test wiki with 10k pages
  bin/cli join                                    # Run entity-wiki join
  bin/cli lucene-build                            # Build Lucene index
  bin/cli lucene-search --query "python web"      # Search index

Environment Variables:
  SPARK_DRIVER_MEMORY     Spark driver memory (default: 6g)
  SPARK_EXECUTOR_MEMORY   Spark executor memory (default: 4g)
  SPARK_MAX_RESULT_SIZE   Max result size (default: 2g)
  PARTITIONS              Number of partitions (default: varies by command)

For subcommand help: bin/cli <subcommand> --help
EOF
}

# =============================================================================
# Subcommand: extract
# =============================================================================

cmd_extract() {
    check_docker

    # Defaults
    local SPARK_ARGS=""
    local SAMPLE=""
    local PARTITIONS="${PARTITIONS:-128}"
    local SHOW_HELP=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            --sample|-s)
                SAMPLE="$2"
                SPARK_ARGS="$SPARK_ARGS --sample $2"
                shift 2
                ;;
            --partitions|-p)
                PARTITIONS="$2"
                shift 2
                ;;
            --force|-f)
                SPARK_ARGS="$SPARK_ARGS --force"
                shift
                ;;
            --dry-run)
                SPARK_ARGS="$SPARK_ARGS --dry-run"
                shift
                ;;
            *)
                SPARK_ARGS="$SPARK_ARGS $1"
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Extract text and entities from crawled GitHub HTML pages.

Usage: bin/cli extract [options]

Options:
  -s, --sample N      Process only first N files (for testing)
  -p, --partitions N  Number of Spark partitions (default: 128)
  -f, --force         Overwrite existing outputs
  --dry-run           List files without processing
  -h, --help          Show this help

Examples:
  bin/cli extract                        # Full extraction
  bin/cli extract --sample 100           # Test with 100 files
  bin/cli extract --force --sample 500   # Force re-extraction of 500 files
EOF
        return 0
    fi

    # Set memory based on data size
    if [[ -n "${SAMPLE}" ]] && [[ "${SAMPLE}" -le 500 ]]; then
        export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-2g}"
        export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-1g}"
        export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-1g}"
    elif [[ -n "${SAMPLE}" ]] && [[ "${SAMPLE}" -le 5000 ]]; then
        export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-4g}"
        export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-2g}"
        export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-2g}"
    else
        export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-6g}"
        export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-4g}"
        export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-2g}"
    fi

    print_header "Spark HTML Extraction"
    echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
    echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
    echo "Partitions: ${PARTITIONS}"
    [[ -n "${SAMPLE}" ]] && echo "Sample Size: ${SAMPLE}"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run  \
        -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
        -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
        -e SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE}" \
        -e PARTITIONS="${PARTITIONS}" \
        -e SPARK_ARGS="${SPARK_ARGS}" \
        spark-extract

    print_success "HTML extraction completed"
}

# =============================================================================
# Subcommand: wiki
# =============================================================================

cmd_wiki() {
    check_docker

    local SHOW_HELP=false
    local SAMPLE_PAGES=""

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            -s|--sample|--wiki-max-pages|--max-wiki-pages)
                SAMPLE_PAGES="$2"
                shift 2
                ;;
            *)
                print_error "Unknown option for wiki: $1"
                echo "Use --help for usage."
                exit 1
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Extract structured data from Wikipedia XML dump (preconfigured Spark settings).

Usage: bin/cli wiki [options]

Options:
  -s, --sample N          Limit wiki pages for a test run (applies --wiki-max-pages)
  -h, --help              Show this help

Outputs:
  workspace/store/wiki/pages.tsv       Page metadata
  workspace/store/wiki/categories.tsv  Page categories
  workspace/store/wiki/links.tsv       Internal links
  workspace/store/wiki/infobox.tsv     Infobox data
  workspace/store/wiki/abstract.tsv    Article abstracts
  workspace/store/wiki/aliases.tsv     Redirect aliases
  workspace/store/wiki/text/           Full article text files (disabled in default config)

Examples:
  bin/cli wiki --sample 10000          # Test with 10k pages
  bin/cli wiki                         # Full extraction (preconfigured)

Preconfigured Spark settings (edit bin/cli to change):
  - Driver Memory: 14g
  - Executor Memory: 8g
  - Python Worker Memory: 4g
  - Network Timeout: 1200s
  - Heartbeat Interval: 60s
  - Partitions: 512
  - Flags: --force-spark --no-text --force
EOF
        return 0
    fi

    # Hardcoded optimized settings for wiki extraction
    export SPARK_DRIVER_MEMORY="14g"
    export SPARK_EXECUTOR_MEMORY="8g"
    export SPARK_MAX_RESULT_SIZE="4g"
    export SPARK_NETWORK_TIMEOUT="1200s"
    export SPARK_EXECUTOR_HEARTBEAT_INTERVAL="60s"
    export SPARK_PYTHON_WORKER_MEMORY="4g"
    export PARTITIONS="512"

    local WIKI_ARGS="--force-spark --no-text --force"

    if [[ -n "${SAMPLE_PAGES}" ]]; then
        WIKI_ARGS="${WIKI_ARGS} --wiki-max-pages ${SAMPLE_PAGES}"
    fi

    print_header "Spark Wikipedia Extraction (Preconfigured)"
    echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
    echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
    echo "Python Worker Memory: ${SPARK_PYTHON_WORKER_MEMORY}"
    echo "Network Timeout: ${SPARK_NETWORK_TIMEOUT}"
    echo "Heartbeat Interval: ${SPARK_EXECUTOR_HEARTBEAT_INTERVAL}"
    echo "Partitions: ${PARTITIONS}"
    echo "Text Extraction: disabled (change in bin/cli to enable)"
    [[ -n "${SAMPLE_PAGES}" ]] && echo "Sample Pages: ${SAMPLE_PAGES}"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run  \
        -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
        -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
        -e SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE}" \
        -e SPARK_NETWORK_TIMEOUT="${SPARK_NETWORK_TIMEOUT}" \
        -e SPARK_EXECUTOR_HEARTBEAT_INTERVAL="${SPARK_EXECUTOR_HEARTBEAT_INTERVAL}" \
        -e SPARK_PYTHON_WORKER_MEMORY="${SPARK_PYTHON_WORKER_MEMORY}" \
        -e PARTITIONS="${PARTITIONS}" \
        -e WIKI_ARGS="${WIKI_ARGS}" \
        spark-wiki

    print_success "Wikipedia extraction completed"
}

# =============================================================================
# Subcommand: join
# =============================================================================

cmd_join() {
    check_docker

    # Defaults
    local JOIN_ARGS=""
    local ENTITIES_MAX_ROWS=""
    local PARTITIONS="${PARTITIONS:-64}"
    local SHOW_HELP=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            --entities-max-rows|-m)
                ENTITIES_MAX_ROWS="$2"
                JOIN_ARGS="$JOIN_ARGS --entities-max-rows $2"
                shift 2
                ;;
            --partitions|-p)
                PARTITIONS="$2"
                shift 2
                ;;
            --dry-run)
                JOIN_ARGS="$JOIN_ARGS --dry-run"
                shift
                ;;
            *)
                JOIN_ARGS="$JOIN_ARGS $1"
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Join extracted entities with Wikipedia data.

Usage: bin/cli join [options]

Options:
  -m, --entities-max-rows N  Limit entity rows (for testing)
  -p, --partitions N         Number of Spark partitions (default: 64)
  --dry-run                  Show what would be joined
  -h, --help                 Show this help

Inputs:
  workspace/store/spark/entities/entities.tsv
  workspace/store/wiki/*.tsv

Outputs:
  workspace/store/join/html_wiki.tsv

Examples:
  bin/cli join                           # Full join
  bin/cli join --entities-max-rows 1000  # Test with 1000 entities
EOF
        return 0
    fi

    export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-6g}"
    export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-3g}"
    export SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE:-2g}"

    print_header "Spark Entity-Wiki Join"
    echo "Driver Memory: ${SPARK_DRIVER_MEMORY}"
    echo "Executor Memory: ${SPARK_EXECUTOR_MEMORY}"
    echo "Partitions: ${PARTITIONS}"
    [[ -n "${ENTITIES_MAX_ROWS}" ]] && echo "Max Entity Rows: ${ENTITIES_MAX_ROWS}"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run \
        -e SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY}" \
        -e SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY}" \
        -e SPARK_MAX_RESULT_SIZE="${SPARK_MAX_RESULT_SIZE}" \
        -e PARTITIONS="${PARTITIONS}" \
        -e JOIN_ARGS="${JOIN_ARGS}" \
        spark-join

    print_success "Entity-Wiki join completed"
}

# =============================================================================
# Subcommand: lucene-build
# =============================================================================

cmd_lucene_build() {
    check_docker

    # Defaults
    local BUILD_ARGS=""
    local LIMIT=""
    local SHOW_HELP=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            --limit|-l)
                LIMIT="$2"
                BUILD_ARGS="$BUILD_ARGS --limit $2"
                shift 2
                ;;
            --dry-run)
                BUILD_ARGS="$BUILD_ARGS --dry-run"
                shift
                ;;
            *)
                BUILD_ARGS="$BUILD_ARGS $1"
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Build PyLucene index from extracted data.

Usage: bin/cli lucene-build [options]

Options:
  -l, --limit N     Limit documents to index (for testing)
  --dry-run         Show what would be indexed
  -h, --help        Show this help

Inputs:
  workspace/store/text/                  Text files
  workspace/store/spark/entities/        Entity data
  workspace/store/join/html_wiki.tsv     Wiki join data

Output:
  workspace/store/lucene_index/          Lucene index

Examples:
  bin/cli lucene-build                   # Build full index
  bin/cli lucene-build --limit 1000      # Build with 1000 docs
EOF
        return 0
    fi

    print_header "PyLucene Index Build"
    echo "Text Dir: workspace/store/text"
    echo "Entities: workspace/store/spark/entities/entities.tsv"
    echo "Wiki Join: workspace/store/join/html_wiki.tsv"
    echo "Output: workspace/store/lucene_index"
    [[ -n "${LIMIT}" ]] && echo "Limit: ${LIMIT}"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run  lucene-build

    print_success "Lucene index build completed"
}

# =============================================================================
# Subcommand: lucene-search
# =============================================================================

cmd_lucene_search() {
    check_docker

    # Defaults
    local QUERY=""
    local QUERY_TYPE="simple"
    local TOP=10
    local SEARCH_ARGS=""
    local SHOW_HELP=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            --query|-q)
                QUERY="$2"
                shift 2
                ;;
            --type|-t)
                QUERY_TYPE="$2"
                shift 2
                ;;
            --top|-n)
                TOP="$2"
                shift 2
                ;;
            --field)
                SEARCH_ARGS="$SEARCH_ARGS --field $2"
                shift 2
                ;;
            --min-value)
                SEARCH_ARGS="$SEARCH_ARGS --min-value $2"
                shift 2
                ;;
            --max-value)
                SEARCH_ARGS="$SEARCH_ARGS --max-value $2"
                shift 2
                ;;
            --slop)
                SEARCH_ARGS="$SEARCH_ARGS --slop $2"
                shift 2
                ;;
            --max-edits)
                SEARCH_ARGS="$SEARCH_ARGS --max-edits $2"
                shift 2
                ;;
            --json)
                SEARCH_ARGS="$SEARCH_ARGS --json"
                shift
                ;;
            *)
                # Treat as query if no query yet
                if [[ -z "${QUERY}" ]]; then
                    QUERY="$1"
                fi
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Search the PyLucene index.

Usage: bin/cli lucene-search [options] [query]

Options:
  -q, --query TEXT    Search query (required)
  -t, --type TYPE     Query type: simple, boolean, range, phrase, fuzzy
  -n, --top N         Number of results (default: 10)
  --field FIELD       Field for range queries
  --min-value N       Min value for range queries
  --max-value N       Max value for range queries
  --slop N            Slop for phrase queries
  --max-edits N       Max edits for fuzzy queries
  --json              Output as JSON
  -h, --help          Show this help

Query Types:
  simple   - Standard keyword search
  boolean  - Boolean operators (AND, OR, NOT)
  phrase   - Exact phrase matching
  fuzzy    - Fuzzy matching for typos
  range    - Numeric range queries

Examples:
  bin/cli lucene-search "python web"
  bin/cli lucene-search --query "python AND docker" --type boolean
  bin/cli lucene-search --query "machine learning" --type phrase
  bin/cli lucene-search --query "pyhton" --type fuzzy
  bin/cli lucene-search --type range --field star_count --min-value 1000
EOF
        return 0
    fi

    if [[ -z "${QUERY}" && "${QUERY_TYPE}" != "range" ]]; then
        print_error "Query is required. Use --query \"search terms\" or --help for usage."
        exit 1
    fi

    print_header "PyLucene Search"
    echo "Query: ${QUERY:-<range query>}"
    echo "Type: ${QUERY_TYPE}"
    echo "Top: ${TOP}"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run  \
        -e QUERY="${QUERY}" \
        -e QUERY_TYPE="${QUERY_TYPE}" \
        -e TOP="${TOP}" \
        lucene-search
}

# =============================================================================
# Subcommand: lucene-compare
# =============================================================================

cmd_lucene_compare() {
    check_docker

    local SHOW_HELP=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            *)
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Compare TF-IDF index vs Lucene index search results.

Usage: bin/cli lucene-compare [options]

Options:
  -h, --help    Show this help

Inputs:
  queries.txt                        Test queries
  workspace/store/index/             TF-IDF index
  workspace/store/lucene_index/      Lucene index

Output:
  reports/index_comparison.md        Comparison report

Examples:
  bin/cli lucene-compare             # Run comparison
EOF
        return 0
    fi

    print_header "Index Comparison: TF-IDF vs Lucene"
    echo "Queries: queries.txt"
    echo "TF-IDF: workspace/store/index"
    echo "Lucene: workspace/store/lucene_index"
    echo "Output: reports/index_comparison.md"
    echo ""

    cd "${PROJECT_ROOT}"
    docker compose -f "${COMPOSE_FILE}" run  lucene-compare

    print_success "Comparison completed"
}

# =============================================================================
# Subcommand: stats
# =============================================================================

cmd_stats() {
    local SHOW_HELP=false
    local OUTPUT_FORMAT="text"
    local REGENERATE=false
    local STATS_DIR="${PROJECT_ROOT}/stats"

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                SHOW_HELP=true
                shift
                ;;
            --json)
                OUTPUT_FORMAT="json"
                shift
                ;;
            --markdown|--md)
                OUTPUT_FORMAT="markdown"
                shift
                ;;
            --regenerate|-r)
                REGENERATE=true
                shift
                ;;
            --dir|-d)
                STATS_DIR="$2"
                shift 2
                ;;
            *)
                shift
                ;;
        esac
    done

    if [[ "$SHOW_HELP" == "true" ]]; then
        cat <<EOF
Show pipeline statistics and generate reports.

Usage: bin/cli stats [options]

Options:
  --json            Output as JSON
  --markdown, --md  Output as Markdown
  -r, --regenerate  Regenerate summary from individual stats
  -d, --dir PATH    Stats directory (default: ./stats)
  -h, --help        Show this help

Stats Files:
  stats/wiki_extraction.json     Wikipedia extraction stats
  stats/html_extraction.json     HTML extraction stats
  stats/join.json                Entity-Wiki join stats
  stats/pipeline_summary.json    Overall pipeline summary
  stats/pipeline_summary.md      Human-readable report

Examples:
  bin/cli stats                  # Show all stats
  bin/cli stats --json           # Output as JSON
  bin/cli stats --markdown       # Output as Markdown report
  bin/cli stats --regenerate     # Regenerate summary
EOF
        return 0
    fi

    print_header "Pipeline Statistics"

    # Check if stats directory exists
    if [[ ! -d "${STATS_DIR}" ]]; then
        print_warning "Stats directory not found: ${STATS_DIR}"
        echo "Run extract/wiki/join/lucene first to generate statistics."
        return 1
    fi

    # If regenerate flag is set, run Python to regenerate summary
    if [[ "$REGENERATE" == "true" ]]; then
        print_warning "Regenerating pipeline summary..."
        cd "${PROJECT_ROOT}"
        python3 -c "
from spark.lib.stats import save_pipeline_summary
from pathlib import Path
json_path, md_path = save_pipeline_summary(Path('${STATS_DIR}'))
print(f'Generated: {json_path}')
print(f'Generated: {md_path}')
"
        echo ""
    fi

    # Output based on format
    case "${OUTPUT_FORMAT}" in
        json)
            if [[ -f "${STATS_DIR}/pipeline_summary.json" ]]; then
                cat "${STATS_DIR}/pipeline_summary.json"
            else
                print_error "No pipeline summary found. Run --regenerate first."
                return 1
            fi
            ;;
        markdown)
            if [[ -f "${STATS_DIR}/pipeline_summary.md" ]]; then
                cat "${STATS_DIR}/pipeline_summary.md"
            else
                print_error "No Markdown report found. Run --regenerate first."
                return 1
            fi
            ;;
        text)
            # Display summary from individual stats files
            echo ""

            # Wiki extraction stats
            if [[ -f "${STATS_DIR}/wiki_extraction.json" ]]; then
                echo -e "${GREEN}Wiki Extraction:${NC}"
                python3 -c "
import json
with open('${STATS_DIR}/wiki_extraction.json') as f:
    s = json.load(f)
    print(f\"  Status: {s.get('status', 'N/A')}\")
    print(f\"  Duration: {s.get('duration_seconds', 0):.2f}s\")
    entities = s.get('entities', {})
    print(f\"  Pages: {entities.get('pages', {}).get('count', 0):,}\")
    print(f\"  Categories: {entities.get('categories', {}).get('count', 0):,}\")
    print(f\"  Links: {entities.get('links', {}).get('count', 0):,}\")
    print(f\"  Infobox Fields: {entities.get('infobox', {}).get('count', 0):,}\")
    print(f\"  Abstracts: {entities.get('abstracts', {}).get('count', 0):,}\")
"
                echo ""
            fi

            # HTML extraction stats
            if [[ -f "${STATS_DIR}/html_extraction.json" ]]; then
                echo -e "${GREEN}HTML Extraction:${NC}"
                python3 -c "
import json
with open('${STATS_DIR}/html_extraction.json') as f:
    s = json.load(f)
    print(f\"  Status: {s.get('status', 'N/A')}\")
    print(f\"  Duration: {s.get('duration_seconds', 0):.2f}s\")
    inputs = s.get('inputs', {})
    outputs = s.get('outputs', {})
    print(f\"  Files Processed: {inputs.get('total_files', 0):,}\")
    print(f\"  Text Files Written: {outputs.get('text_files_written', 0):,}\")
    print(f\"  Entities Extracted: {outputs.get('entities_written', 0):,}\")
"
                echo ""
            fi

            # Join stats
            if [[ -f "${STATS_DIR}/join.json" ]]; then
                echo -e "${GREEN}Entity-Wiki Join:${NC}"
                python3 -c "
import json
with open('${STATS_DIR}/join.json') as f:
    s = json.load(f)
    print(f\"  Status: {s.get('status', 'N/A')}\")
    print(f\"  Duration: {s.get('duration_seconds', 0):.2f}s\")
    js = s.get('join_statistics', {})
    print(f\"  Total Entities: {js.get('total_entities', 0):,}\")
    print(f\"  Matched Entities: {js.get('matched_entities', 0):,}\")
    print(f\"  Match Rate: {js.get('match_rate', 0):.2f}%\")
    print(f\"  Unique Wiki Pages: {js.get('unique_wiki_pages_joined', 0):,}\")
"
                echo ""
            fi

            # Overall summary
            if [[ -f "${STATS_DIR}/pipeline_summary.json" ]]; then
                echo -e "${BLUE}─────────────────────────────────────${NC}"
                echo -e "${BLUE}Pipeline Summary:${NC}"
                python3 -c "
import json
with open('${STATS_DIR}/pipeline_summary.json') as f:
    s = json.load(f)
    totals = s.get('totals', {})
    print(f\"  Overall Status: {totals.get('overall_status', 'N/A')}\")
    print(f\"  Total Duration: {totals.get('total_duration_seconds', 0):.2f}s\")
    print(f\"  Wiki Pages: {totals.get('total_wiki_pages', 0):,}\")
    print(f\"  HTML Files: {totals.get('total_html_files', 0):,}\")
    print(f\"  Entities Extracted: {totals.get('total_entities_extracted', 0):,}\")
    print(f\"  Total Joins: {totals.get('total_joins', 0):,}\")
"
            else
                print_warning "No pipeline summary found. Run 'bin/cli stats --regenerate' to generate."
            fi
            ;;
    esac
}

# =============================================================================
# Main Entry Point
# =============================================================================

main() {
    if [[ $# -eq 0 ]]; then
        show_main_help
        exit 0
    fi

    local SUBCOMMAND="$1"
    shift

    case "${SUBCOMMAND}" in
        extract)
            cmd_extract "$@"
            ;;
        wiki)
            cmd_wiki "$@"
            ;;
        join)
            cmd_join "$@"
            ;;
        lucene-build)
            cmd_lucene_build "$@"
            ;;
        lucene-search)
            cmd_lucene_search "$@"
            ;;
        lucene-compare)
            cmd_lucene_compare "$@"
            ;;
        stats)
            cmd_stats "$@"
            ;;
        -h|--help)
            show_main_help
            ;;
        --version)
            echo "VINF CLI v1.0.0"
            ;;
        *)
            print_error "Unknown subcommand: ${SUBCOMMAND}"
            echo ""
            show_main_help
            exit 1
            ;;
    esac
}

main "$@"
