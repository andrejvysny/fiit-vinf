# Unified Crawler Configuration (Refactored)
# Single crawler with integrated fetch+store (no separate scraper needed)

run_id: "unified-2025-01-12"
workspace: "./workspace"
seeds:
  - https://github.com/python/cpython
  - https://github.com/torvalds/linux
  - https://github.com/tensorflow/tensorflow
  - https://github.com/kubernetes/kubernetes
  - https://github.com/facebook/react
  - https://github.com/vuejs/vue
  - https://github.com/golang/go
  - https://github.com/rust-lang/rust
  - https://github.com/microsoft/vscode
  - https://github.com/nodejs/node

# User agent MUST include contact information
# You can provide a single user_agent or a list 'user_agents' to rotate per request.
#user_agent: "ResearchCrawlerBotVINF/2.0 (+mailto:xvysnya@stuba.sk)"
user_agents:
  - "ResearchCrawlerBotVINF/2.0 (+mailto:xvysnya@stuba.sk)"
  - Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0
  - Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0
user_agent_rotation_size: 3

accept_language: "en"
accept_encoding: "br, gzip"

# Robots.txt configuration
robots:
  user_agent: "ResearchCrawlerBotVINF/2.0 (+mailto:xvysnya@stuba.sk)"
  cache_ttl_sec: 86400  # 24 hours

# URL scope - STRICT deny-by-default
scope:
  # Only github.com allowed (no subdomains)
  allowed_hosts: ["github.com"]
  
  # Explicitly denied subdomains
  denied_subdomains:
    - "api.github.com"
    - "raw.githubusercontent.com"
    - "gist.github.com"
    - "codeload.github.com"
  
  # Allowlist patterns (regex) - ONLY these are allowed
  allow_patterns:
    - "^https://github\\.com/topics(?:/[^/?#]+)?$"           # Topics pages
    - "^https://github\\.com/trending$"                      # Trending (no params)
    - "^https://github\\.com/[^/]+/[^/]+/?$"                # Repo root
   # - "^https://github\\.com/[^/]+/[^/]+/blob/[^?#]+$"      # Blob viewer
    - "^https://github\\.com/[^/]+/[^/]+/(issues|pull)/?\\d*$"  # Issues/PRs
  
  # Deny patterns - explicitly blocked (checked after allow)
  deny_patterns:
    - "/blob/"      # Blob viewer (large files)
    - "/search"
    - "/tree/"
    - "/commits/"
    - "/graphs/"
    - "/compare/"
    - "/archive/"
    - "/blame/"
    - "/raw/"
    - "/network/"
    - "/stargazers"
    - "/watchers"
    - "/contributors"
    - "/tags"
    - "/branches"
    - "/tarball/"
    - "/zipball/"
    - "\\?q="       # Search queries
    - "\\?tab="     # Tab parameters
    - "\\?since="   # Time filters
    - "\\?until="   # Time filters
    - "/resources/"  # Non-HTML resources
  
  # Only HTML content
  content_types: ["text/html"]

# Rate limiting - conservative settings
limits:
  global_concurrency: 4        # Max concurrent requests
  per_host_concurrency: 2      # Max per host
  req_per_sec: 1.0            # Target request rate (1 req/sec)
  connect_timeout_ms: 4000    # 4 seconds
  read_timeout_ms: 15000      # 15 seconds
  total_timeout_ms: 25000     # 25 seconds total
  max_retries: 3
  backoff_base_ms: 500        # Start backoff
  backoff_cap_ms: 8000        # Max backoff

# Crawl depth and per-repository caps
caps:
  per_repo_max_pages: 30      # Max pages per repository
  per_repo_max_issues: 10     # Max issues per repository
  per_repo_max_prs: 10        # Max pull requests per repository
  max_depth: 4                # Maximum crawl depth

# Dynamic sleep/pause configuration for human-like behavior
sleep:
  # Sleep after each request (in seconds) - random value in range
  per_request_min: 3
  per_request_max: 5
  
  # Batch pause after N requests (in seconds) - random value in range
  batch_pause_min: 10
  batch_pause_max: 20
  
  # Batch size - number of requests before applying batch pause
  batch_size: 50

# File-based storage paths (all relative to workspace)
# NO LMDB - all file-based!
storage:
  # Frontier (URL queue)
  frontier_file: "state/frontier.jsonl"
  
  # Dedup (fetched URLs tracking)
  fetched_urls_file: "state/fetched_urls.txt"
  
  # Robots cache
  robots_cache_file: "state/robots_cache.jsonl"
  
  # Unified metadata (crawl + scrape combined)
  metadata_file: "metadata/crawl_metadata.jsonl"
  
  # HTML content-addressed store
  html_store_root: "store/html"
  html_compress: false
  html_permissions: 0o644

# Logging
logs:
  log_file: "logs/crawler.log"
  log_level: "INFO"
