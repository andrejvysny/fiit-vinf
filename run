#!/usr/bin/env python3
"""
run - CLI wrapper for the unified crawler.

This wrapper provides commands for the refactored single-fetch architecture:
  - run       : Start unified crawler (fetch + store in one operation)
  - configure : Create workspace directories
  - reset     : Remove all workspace data
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
from pathlib import Path


REPO_ROOT = Path(__file__).resolve().parent
WORKSPACE = REPO_ROOT / "workspace"
STATE_DIR = WORKSPACE / "state"
METADATA_DIR = WORKSPACE / "metadata"
STORE_DIR = WORKSPACE / "store" / "html"
LOGS_DIR = WORKSPACE / "logs"
SEEDS_FILE = REPO_ROOT / "seeds.txt"


def _find_python_executable() -> str:
    """Return path to Python executable."""
    venv_py = REPO_ROOT / "venv" / "bin" / "python3"
    if venv_py.exists():
        return str(venv_py)
    venv_py_alt = REPO_ROOT / "venv" / "bin" / "python"
    if venv_py_alt.exists():
        return str(venv_py_alt)
    return sys.executable


PYTHON_EXE = _find_python_executable()


def _workspace_configured() -> bool:
    """Return True when workspace scaffolding already exists."""
    required_dirs = (STATE_DIR, METADATA_DIR, STORE_DIR, LOGS_DIR)
    if not all(path.is_dir() for path in required_dirs):
        return False
    return SEEDS_FILE.exists()


def configure(args=None):
    """Create workspace directories for unified crawler."""
    STATE_DIR.mkdir(parents=True, exist_ok=True)
    METADATA_DIR.mkdir(parents=True, exist_ok=True)
    STORE_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    
    # Ensure default seeds file exists
    if not SEEDS_FILE.exists():
        default_seeds = """# GitHub Crawler Seeds
# One URL per line, comments start with #
https://github.com/
"""
        SEEDS_FILE.write_text(default_seeds)
        print(f"Created default seeds file: {SEEDS_FILE}")
    
    print(f"Workspace directories configured under {WORKSPACE}")
    print(f"  - State: {STATE_DIR}")
    print(f"  - Metadata: {METADATA_DIR}")
    print(f"  - HTML Store: {STORE_DIR}")
    print(f"  - Logs: {LOGS_DIR}")


def reset(args=None):
    """Remove all workspace data after confirmation."""
    print("This will REMOVE ALL crawled data:")
    print(f"  - State files (frontier, dedup, robots cache): {STATE_DIR}")
    print(f"  - Metadata: {METADATA_DIR}")
    print(f"  - HTML store: {STORE_DIR}")
    print(f"  - Logs: {LOGS_DIR}")
    print()
    answer = input("Type 'yes' to confirm: ")
    
    if answer.strip().lower() != "yes":
        print("Aborted.")
        return
    
    # Remove contents but keep directories
    def rm_contents(path: Path):
        if path.exists():
            for child in path.iterdir():
                try:
                    if child.is_dir():
                        shutil.rmtree(child)
                    else:
                        child.unlink()
                    print(f"Removed: {child}")
                except Exception as e:
                    print(f"Failed to remove {child}: {e}")
    
    rm_contents(STATE_DIR)
    rm_contents(METADATA_DIR)
    rm_contents(STORE_DIR)
    rm_contents(LOGS_DIR)
    
    print("\nWorkspace reset complete.")


def run(args):
    """Run unified crawler."""
    if not _workspace_configured():
        configure()
    
    config_path = args.config if args.config else str(REPO_ROOT / "config.yaml")
    seeds_path = args.seeds if args.seeds else str(SEEDS_FILE)
    
    # Build command
    cmd = [PYTHON_EXE, str(REPO_ROOT / "main.py"), 
           "--config", config_path,
           "--seeds", seeds_path]
    
    print(f"Starting unified crawler: {' '.join(cmd)}\n")
    
    try:
        # Run in foreground
        rc = subprocess.call(cmd)
        print(f"\nCrawler exited with code: {rc}")
    except KeyboardInterrupt:
        print("\n\nInterrupted by user")


def main(argv=None):
    parser = argparse.ArgumentParser(
        prog="vi-scrape-unified",
        description="Unified crawler wrapper - single fetch architecture"
    )
    
    sub = parser.add_subparsers(dest="cmd")
    
    # Run command
    p_run = sub.add_parser("run", help="Start unified crawler")
    p_run.add_argument("--config", help="Config path (default: config.yaml)")
    p_run.add_argument("--seeds", help="Seeds file path (default: seeds.txt)")
    
    # Configure command
    sub.add_parser("configure", help="Create workspace directories (idempotent)")
    
    # Reset command
    sub.add_parser("reset", help="Remove all workspace data (interactive)")
    
    args = parser.parse_args(argv)
    
    if not args.cmd:
        parser.print_help()
        sys.exit(1)
    
    if args.cmd == "run":
        run(args)
    elif args.cmd == "configure":
        configure(args)
    elif args.cmd == "reset":
        reset(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
